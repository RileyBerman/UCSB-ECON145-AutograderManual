# Autograder Conventions {#Autograder-Conventions}

The *autograder infrastructure*, or *autograder system*, is a set of R scripts that work together to implement the *autograder* -- that is, to evaluate a student's R-assignment and return question-specific feedback through the Gradescope platform. 

Sometimes, the term "autograder" is used to refer to the entire system, but I will try to stick to this division throughout the manual.  

:::: {.infobox .imp data-latex="{important_exclamation}"}

**WHY FOCUS ON JUST THE AUTOGRADER?**

TAs primarily work on writing the autograder, while the Head TA handles the integration of the autograder and its infastructure with Gradescope.

::::

A quick overview of the [DGP.R] file will also be provided, which *every TA* should review.

For implementation and testing details of the autograder, refer to the document[`how_to_create_an_autograder_2024.pdf`](#Resources). 

## General Structure 

Generally, an autograder script will look something like: 

```{r, eval=F}

rm(list = ls())

#--------------Set This------#
#loc       <- "local" # either "local", or "gradescope"
loc       <- "gradescope"

#------DON'T TOUCH THIS------#

#Setting working directory to source file location
if(loc=="local"){
  setwd(dirname(rstudioapi::getSourceEditorContext()$path))
}
source("inputs.R")
source(paste0(here::here(),"/helper_functions/autograder_setup.R",""))
source(paste0(here::here(),"/helper_functions/misc_helper_functions.R",""))
source("inputs.R")

#----------------------------#

if(status!="Error"){
  
  #Answer Key Goes Here...  
  
  #Question 1 Solution
  
  #Question 2 Solution, etc. 
  
  #Autograder Code Goes Here... 
  
  #Testing Student's Question 1 Against Question 1 Solution
  
  #Testing Student's Question 2 Against Question 2 Solution, etc.  
  
  # ----------------------------------------------------------- #
  
  JSONmaker(test.results, loc)
}
```

Under `#Answer Key Goes Here...`, TAs insert the assignment solutions. To distinguish solutions from the student's answers (so they can be compared), we tend to follow the convention of appending "`_test`" to the answer key's variable name. 

-   For example, if Question 1 on the homework asks the student to create a tibble named `basketball_data`, the answer key's corresponding tibble will be named `basketball_data_test`.

Under `#Autograder Code Goes Here...`, TAs insert their code for the autograder. Question by question, this code will compare the student's solution to the corresponding answer key. As a reminder... 

-   For any Public Question, we program a series of "Checks" (lines of code) to test the student's answer for a *range of attributes* and return dynamic feedback. See [Public Questions](#Public-Questions). 

-   For any Private Question, we employ a group of built-in functions to test the student's answer for a *fixed set of attributes* and return basic feedback. See [Private Questions](#Private-Questions). 

:::: {.infobox .warning data-latex="{warning}"}

The autograder **only** evaluates the student's final result, the R object -- it does not grade the code required to produce it. 

Accordingly, students can receive full credit for a question even if their code is vastly different from the answer key. The feedback our Checks generate guide students towards the *recommended, class-based* solution. 

The student's R object **does not** need to be identical to the answer key to receive full credit. See [Correct Check](#Correct-Check) for more details. 

:::: 

## What is test.results? {#test.results}

The autograder infrastructure has provided a `test.results` data frame to store and display feedback from our Checks. 

When writing autograder code for *any* question, we *always* first initialize the question's default `test.results`. 

For example, to initialize Question 1's `test.results`, we write the following line:

```{r, eval=F}

#Testing Student's Question 1 Against Question 1 Solution
test.results[1, ] <- c("Part 1 Question 1 (Public)", 0, 20, "Try again.")

```

where each element of `test.results[#, ]` is as follows:

\renewcommand{\arraystretch}{2}

```{r table0, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

data = data.frame(
  `Element` = c("1", "\"Part 1 Question 1 (Public)\"", "0", "20", "\"Try again.\""),
  `Description` = c("the question's number", 
                    "the question's displayed part, number and type (Private/Public)", 
                    "the question's default score", 
                    "the question's maximum score", 
                    "the default feedback message"))

library(tidyverse)

if (knitr::is_html_output()) {
  knitr::kable(data, booktabs = TRUE, longtable = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
} else { 
  knitr::kable(data, format = "latex", booktabs = TRUE, longtable = TRUE, linesep = "\\hline") |>
    kableExtra::kable_styling(latex_options = c("striped")) |> 
    kableExtra::row_spec(0, bold = TRUE)  
}

```

 It is important to note that a question's *displayed number* may differ from its *question number*, especially in the case of multi-part questions. 

**Recommendation**: If students would greatly benefit from a very specific hint (e.g., using a certain function), you can include it in the default feedback message to ensure they see it in the case that none of the Checks trigger. 

After initializing a question's `test.results[#, ]`, we can update it through indexing. 

-   For example, if the student's Question 1 is correct, we can update `test.results[1, ]` to award them full score with the following line:

```{r, eval=FALSE}

#Modifying the score for Question 1
test.results[1, 2] <- 20

```

Similarly, we can modify the feedback message through the following line: 

```{r, eval=F}

#Modifying the feedback message for Question 1
test.results[1, 4] <- "Insert feedback message here!"

```

:::: {.infobox .note data-latex="{bell_note}"}

**Note**: 

For most of the sample code provided for [Public Questions](#Public-Questions), we will be updating `test.results[2, ]` (i.e., Question 2). 

::::
